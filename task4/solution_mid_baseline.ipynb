{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n",
    "# First, we import necessary libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RBF, Matern, WhiteKernel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x180a8ebb270>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"public/pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                         axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"public/pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"public/train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                   axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"public/train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"public/test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraining data\n",
    "        # and then used to extract features from the training and test data.\n",
    "        #self.fully_con1 = nn.Sequential(nn.Linear(1000, 128), nn.ReLU())\n",
    "        self.fully_con1 = nn.Linear(1000, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture\n",
    "        # defined in the constructor.\n",
    "        x = self.fully_con1(x)\n",
    "        #x = self.fully_con2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "\n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.train()\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set\n",
    "    # to monitor the loss.\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) # 1e-2\n",
    "    n_epochs = 5\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_tr, y_tr),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = []\n",
    "        valid_loss_epoch = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} train\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predictions = model(data).squeeze()\n",
    "                loss = criterion(predictions, target)\n",
    "                train_loss_epoch.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss_avg = np.sum(train_loss_epoch) / len(train_loss_epoch)\n",
    "                tepoch.set_postfix({'Train loss': train_loss_avg})\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "                    for valid_data, valid_target in tepoch:\n",
    "                        tepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "                        valid_predictions = model(valid_data).squeeze()\n",
    "                        valid_loss = criterion(valid_predictions, valid_target)\n",
    "                        valid_loss_epoch.append(valid_loss.item())\n",
    "\n",
    "                        valid_loss_avg = np.sum(valid_loss_epoch) / len(valid_loss_epoch)\n",
    "                        tepoch.set_postfix({'Validation loss': valid_loss.item()})\n",
    "\n",
    "        losses.append(train_loss_avg)\n",
    "        valid_losses.append(valid_loss_avg)\n",
    "\n",
    "        print('Final train loss: ', train_loss_avg, 'Final valid loss: ', valid_loss_avg)\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline\n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        model_no_last_layer = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, pd.DataFrame):\n",
    "                x = x.to_numpy()\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            x_features = model_no_last_layer(x)\n",
    "\n",
    "        return x_features\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "\n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "\n",
    "    return PretrainedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_regression_model():\n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    \n",
    "    # model = Ridge(alpha=0)\n",
    "    \n",
    "    model = GaussianProcessRegressor(optimizer='fmin_l_bfgs_b', random_state=None, n_restarts_optimizer=0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train: 100%|██████████| 192/192 [00:00<00:00, 202.54batch/s, Train loss=0.324]\n",
      "Epoch 0 valid: 100%|██████████| 4/4 [00:00<00:00, 133.34batch/s, Validation loss=0.0799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.3243788054484564 Final valid loss:  0.05493928864598274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|██████████| 192/192 [00:00<00:00, 204.26batch/s, Train loss=0.0503]\n",
      "Epoch 1 valid: 100%|██████████| 4/4 [00:00<00:00, 114.28batch/s, Validation loss=0.0504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.050292948338513575 Final valid loss:  0.045554437674582005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|██████████| 192/192 [00:00<00:00, 204.69batch/s, Train loss=0.0418]\n",
      "Epoch 2 valid: 100%|██████████| 4/4 [00:00<00:00, 137.93batch/s, Validation loss=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.04183835885487497 Final valid loss:  0.03759887535125017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|██████████| 192/192 [00:00<00:00, 205.35batch/s, Train loss=0.0357]\n",
      "Epoch 3 valid: 100%|██████████| 4/4 [00:00<00:00, 142.85batch/s, Validation loss=0.0321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.03569615497447861 Final valid loss:  0.030999042559415102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|██████████| 192/192 [00:00<00:00, 215.01batch/s, Train loss=0.0309]\n",
      "Epoch 4 valid: 100%|██████████| 4/4 [00:00<00:00, 142.85batch/s, Validation loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.030930804001400247 Final valid loss:  0.028341423720121384\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "{'alpha': 0.001, 'kernel': Matern(length_scale=0.8, nu=1.5) + WhiteKernel(noise_level=1)}\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy\n",
    "# features from available initial features\n",
    "feature_extractor = make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "\n",
    "# regression model\n",
    "regression_model = get_regression_model()\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "x_train_transformed = feature_extractor(x_train).numpy()\n",
    "x_test_transformed = feature_extractor(x_test).numpy()\n",
    "\n",
    "param_range = [np.arange(0, 1.0, 0.2), np.arange(0, 1.0, 0.2)]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "    \"alpha\":  [1e-2, 1e-3, 1e-3],\n",
    "    \"kernel\": [Matern(length_scale, nu=1.5) + WhiteKernel() for length_scale in np.arange(0, 1.0, 0.1)]}# list(itertools.product(*param_range))]}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(estimator=regression_model, param_grid=param_grid, cv=5, refit=True, n_jobs = 5, verbose=True,\n",
    "                       scoring='%s' % 'neg_root_mean_squared_error')\n",
    "\n",
    "clf.fit(x_train_transformed, y_train)\n",
    "regression_model = clf.best_estimator_\n",
    "print(clf.best_params_)\n",
    "\n",
    "regression_model.fit(x_train_transformed, y_train)\n",
    "y_pred = regression_model.predict(x_test_transformed)\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(\"results.csv\", index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
