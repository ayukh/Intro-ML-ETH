{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n",
    "# First, we import necessary libraries:\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.linear_model import LinearRegression, Lasso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x24f06e1cc90>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"public/pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                         axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"public/pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"public/train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                   axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"public/train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"public/test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraining data\n",
    "        # and then used to extract features from the training and test data.\n",
    "\n",
    "        # activation = nn.ReLU()\n",
    "\n",
    "        # THINGS TO INCLUDE: BATCH NORM, DROPOUT\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 500), nn.BatchNorm1d(500), nn.ReLU(),\n",
    "            nn.Linear(500, 250), nn.BatchNorm1d(250), nn.ReLU(),\n",
    "            nn.Linear(250, 100), nn.BatchNorm1d(100),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(100, 50), nn.ReLU(),\n",
    "            # nn.Linear(50, 10), nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(10, 50), nn.ReLU(),\n",
    "            # nn.Linear(50, 100), nn.ReLU(),\n",
    "            nn.Linear(100, 250), nn.BatchNorm1d(250), nn.ReLU(),\n",
    "            nn.Linear(250, 500), nn.BatchNorm1d(500), nn.ReLU(),\n",
    "            nn.Linear(500, 1000)\n",
    "        )\n",
    "        self.last_linear = nn.Sequential(nn.Linear(100, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture\n",
    "        # defined in the constructor.\n",
    "        x = self.encoder(x)\n",
    "        y = self.last_linear(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "\n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.train()\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set\n",
    "    # to monitor the loss.\n",
    "\n",
    "    criterion_decoded = nn.MSELoss()\n",
    "    criterion_predictions = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    n_epochs = 5\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_tr, y_tr),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = []\n",
    "        valid_loss_epoch = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} train\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                decoded_features, predictions = model(data)\n",
    "                predictions = predictions.squeeze()\n",
    "\n",
    "                loss_decoded = criterion_decoded(decoded_features, data)\n",
    "                loss_predictions = criterion_predictions(predictions, target)\n",
    "\n",
    "                a = 0.1\n",
    "                train_loss = a*loss_predictions + (1-a)*loss_decoded\n",
    "\n",
    "                train_loss_epoch.append(train_loss.item())\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tepoch.set_postfix({'Train loss': train_loss.item()})\n",
    "\n",
    "        train_loss_avg = np.mean(train_loss_epoch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "                for valid_data, valid_target in tepoch:\n",
    "                    tepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "\n",
    "                    valid_decoded_features, valid_predictions = model(valid_data)\n",
    "                    valid_predictions = valid_predictions.squeeze()\n",
    "\n",
    "                    valid_loss_decoded = criterion_decoded(valid_decoded_features, valid_data)\n",
    "                    valid_loss_predictions = criterion_predictions(valid_predictions, valid_target)\n",
    "\n",
    "                    valid_loss = a*valid_loss_predictions + (1-a)*valid_loss_decoded\n",
    "\n",
    "                    valid_loss_epoch.append(valid_loss.item())\n",
    "\n",
    "                    tepoch.set_postfix({'Validation loss': valid_loss.item()})\n",
    "\n",
    "        valid_loss_avg = np.mean(valid_loss_epoch)\n",
    "\n",
    "\n",
    "        losses.append(train_loss_avg)\n",
    "        valid_losses.append(valid_loss_avg)\n",
    "\n",
    "        print('Final train loss: ', train_loss_avg, 'Final valid loss: ', valid_loss_avg)\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline\n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        model_no_last_layers = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, pd.DataFrame):\n",
    "                x = x.to_numpy()\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            x_features = model_no_last_layers(x)\n",
    "\n",
    "        return x_features\n",
    "\n",
    "    return make_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train: 100%|██████████| 192/192 [00:05<00:00, 36.00batch/s, Train loss=0.0272]\n",
      "Epoch 0 valid: 100%|██████████| 4/4 [00:00<00:00, 114.29batch/s, Validation loss=0.0274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.21882997035087706 Final valid loss:  0.027624611277133226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|██████████| 192/192 [00:04<00:00, 42.24batch/s, Train loss=0.0248]\n",
      "Epoch 1 valid: 100%|██████████| 4/4 [00:00<00:00, 107.74batch/s, Validation loss=0.0226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.024823771566540625 Final valid loss:  0.022794404067099094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|██████████| 192/192 [00:04<00:00, 41.89batch/s, Train loss=0.0197]\n",
      "Epoch 2 valid: 100%|██████████| 4/4 [00:00<00:00, 102.57batch/s, Validation loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.020860203803749755 Final valid loss:  0.019452263135463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|██████████| 192/192 [00:04<00:00, 41.40batch/s, Train loss=0.018] \n",
      "Epoch 3 valid: 100%|██████████| 4/4 [00:00<00:00, 117.64batch/s, Validation loss=0.0171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.018026713747531176 Final valid loss:  0.017161923460662365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|██████████| 192/192 [00:04<00:00, 40.69batch/s, Train loss=0.0153]\n",
      "Epoch 4 valid: 100%|██████████| 4/4 [00:00<00:00, 111.09batch/s, Validation loss=0.0148]\n",
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.015698807740894456 Final valid loss:  0.0148190101608634\n"
     ]
    }
   ],
   "source": [
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy\n",
    "# features from available initial features\n",
    "feature_extractor = make_feature_extractor(x_pretrain, y_pretrain)\n",
    "\n",
    "x_train_transformed = feature_extractor(x_train).numpy()\n",
    "x_test_transformed = feature_extractor(x_test).numpy()\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "# STANDARDSCALER, FUNCTIONTRANSFORMER, etc.\n",
    "x_train_transformed = scale(x_train_transformed)\n",
    "x_test_transformed = scale(x_test_transformed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0\n",
      "{'fit_time': array([0.04048324, 0.00200081, 0.0030005 , 0.00300002, 0.00199986]),\n",
      " 'score_time': array([0.00200033, 0.        , 0.00099969, 0.        , 0.        ]),\n",
      " 'test_score': array([-0.27480896, -0.3330765 , -0.40158268, -0.24590953, -0.3125876 ]),\n",
      " 'train_score': array([-9.16513608e-06, -1.73120890e-06, -1.86957990e-05, -4.81487835e-06,\n",
      "       -1.04403853e-05])}\n",
      "alpha = 0.001\n",
      "{'fit_time': array([0.01100492, 0.00101376, 0.00099945, 0.00098038, 0.00101662]),\n",
      " 'score_time': array([0.00202036, 0.0010004 , 0.        , 0.        , 0.0010016 ]),\n",
      " 'test_score': array([-0.24016498, -0.28581638, -0.37161898, -0.21469307, -0.29031317]),\n",
      " 'train_score': array([-0.0101706 , -0.01134562, -0.0087398 , -0.01042438, -0.00882693])}\n",
      "alpha = 0.01\n",
      "{'fit_time': array([0.00098181, 0.0009973 , 0.00199962, 0.00100565, 0.00100136]),\n",
      " 'score_time': array([0.0010221, 0.0010078, 0.       , 0.       , 0.       ]),\n",
      " 'test_score': array([-0.20035152, -0.23958591, -0.25752826, -0.19343458, -0.25583726]),\n",
      " 'train_score': array([-0.03699701, -0.03524888, -0.0358291 , -0.03905093, -0.03318819])}\n",
      "alpha = 0.1\n",
      "{'fit_time': array([0.0010016 , 0.00100017, 0.00099945, 0.00099897, 0.        ]),\n",
      " 'score_time': array([0.00099897, 0.        , 0.        , 0.        , 0.00099897]),\n",
      " 'test_score': array([-0.17547333, -0.20146526, -0.18585928, -0.19904331, -0.23105367]),\n",
      " 'train_score': array([-0.07845572, -0.0730081 , -0.07889924, -0.07978417, -0.07431296])}\n",
      "alpha = 1\n",
      "{'fit_time': array([0.00099945, 0.00099969, 0.00100493, 0.00099587, 0.00100398]),\n",
      " 'score_time': array([0., 0., 0., 0., 0.]),\n",
      " 'test_score': array([-0.18117081, -0.16681312, -0.20472028, -0.19712573, -0.23379747]),\n",
      " 'train_score': array([-0.11943916, -0.11942236, -0.11312809, -0.11724934, -0.11650413])}\n",
      "alpha = 10\n",
      "{'fit_time': array([0.0010066 , 0.        , 0.00100017, 0.00099969, 0.        ]),\n",
      " 'score_time': array([0.        , 0.00099945, 0.        , 0.        , 0.        ]),\n",
      " 'test_score': array([-0.23055553, -0.16765886, -0.22842327, -0.18928094, -0.23697718]),\n",
      " 'train_score': array([-0.158891  , -0.16613474, -0.15549962, -0.15940759, -0.1546397 ])}\n",
      "alpha = 100\n",
      "{'fit_time': array([0.        , 0.00099945, 0.0010004 , 0.00099993, 0.00099993]),\n",
      " 'score_time': array([0.00100017, 0.        , 0.        , 0.        , 0.        ]),\n",
      " 'test_score': array([-0.31310222, -0.22451523, -0.27221544, -0.20133028, -0.25937155]),\n",
      " 'train_score': array([-0.22149905, -0.23504064, -0.22582798, -0.23662095, -0.22753549])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.15239e-08): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.60894e-08): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.01314e-08): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "C:\\Users\\Gustas\\anaconda3\\envs\\iml-2023\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.87656e-08): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n"
     ]
    }
   ],
   "source": [
    "print('alpha =', 0)\n",
    "pprint(cross_validate(LinearRegression(), x_train_transformed, y_train, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True))\n",
    "for alpha in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    print('alpha =', alpha)\n",
    "    pprint(cross_validate(Ridge(alpha=alpha), x_train_transformed, y_train, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00200248, 0.00101161, 0.00101757, 0.0010035 , 0.00105572]),\n",
      " 'score_time': array([0.00103378, 0.00099587, 0.00098181, 0.        , 0.        ]),\n",
      " 'test_score': array([-0.20163826, -0.16007141, -0.21327105, -0.19572012, -0.23694184]),\n",
      " 'train_score': array([-0.13733562, -0.14077238, -0.13100312, -0.13588726, -0.13342928])}\n"
     ]
    }
   ],
   "source": [
    "pprint(cross_validate(Ridge(alpha=3), x_train_transformed, y_train, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "regression_model = Ridge(alpha=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "regression_model.fit(x_train_transformed, y_train)\n",
    "y_pred = regression_model.predict(x_test_transformed)\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(\"results.csv\", index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "iml-2023",
   "language": "python",
   "display_name": "Python (iml-2023)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}