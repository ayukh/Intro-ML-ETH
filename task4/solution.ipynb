{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n",
    "# First, we import necessary libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x27a12169c90>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"public/pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                         axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"public/pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"public/train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                   axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"public/train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"public/test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraining data\n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fully_con1 = nn.Sequential(nn.Linear(1000, 99), nn.ReLU())\n",
    "        self.fully_con2 = nn.Linear(99, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture\n",
    "        # defined in the constructor.\n",
    "        x = self.fully_con1(x)\n",
    "        x = self.fully_con2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "\n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.train()\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set\n",
    "    # to monitor the loss.\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    n_epochs = 5\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_tr, y_tr),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = []\n",
    "        valid_loss_epoch = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} train\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predictions = model(data).squeeze()\n",
    "                loss = criterion(predictions, target)\n",
    "                train_loss_epoch.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss_avg = np.sum(train_loss_epoch) / len(train_loss_epoch)\n",
    "                tepoch.set_postfix({'Train loss': train_loss_avg})\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "                    for valid_data, valid_target in tepoch:\n",
    "                        tepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "                        valid_predictions = model(valid_data).squeeze()\n",
    "                        valid_loss = criterion(valid_predictions, valid_target)\n",
    "                        valid_loss_epoch.append(valid_loss.item())\n",
    "\n",
    "                        valid_loss_avg = np.sum(valid_loss_epoch) / len(valid_loss_epoch)\n",
    "                        tepoch.set_postfix({'Validation loss': valid_loss.item()})\n",
    "\n",
    "        losses.append(train_loss_avg)\n",
    "        valid_losses.append(valid_loss_avg)\n",
    "\n",
    "        print('Final train loss: ', train_loss_avg, 'Final valid loss: ', valid_loss_avg)\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline\n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        model_no_last_layer = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, pd.DataFrame):\n",
    "                x = x.to_numpy()\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            x_features = model_no_last_layer(x)\n",
    "\n",
    "        return x_features\n",
    "\n",
    "    return make_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "\n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "\n",
    "    return PretrainedFeatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_regression_model():\n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Ridge(alpha=0)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train: 100%|██████████| 192/192 [00:01<00:00, 143.28batch/s, Train loss=0.224]\n",
      "Epoch 0 valid: 100%|██████████| 4/4 [00:00<00:00, 184.57batch/s, Validation loss=0.045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.223555719217984 Final valid loss:  0.04618350602686405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|██████████| 192/192 [00:01<00:00, 156.74batch/s, Train loss=0.0402]\n",
      "Epoch 1 valid: 100%|██████████| 4/4 [00:00<00:00, 190.46batch/s, Validation loss=0.0291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.04022655988228507 Final valid loss:  0.03211171831935644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|██████████| 192/192 [00:01<00:00, 151.82batch/s, Train loss=0.0254]\n",
      "Epoch 2 valid: 100%|██████████| 4/4 [00:00<00:00, 190.46batch/s, Validation loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.02538299093430396 Final valid loss:  0.020648547913879156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|██████████| 192/192 [00:01<00:00, 156.05batch/s, Train loss=0.0157]\n",
      "Epoch 3 valid: 100%|██████████| 4/4 [00:00<00:00, 196.49batch/s, Validation loss=0.0133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.015738252380591195 Final valid loss:  0.014122535474598408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|██████████| 192/192 [00:01<00:00, 141.25batch/s, Train loss=0.0119]\n",
      "Epoch 4 valid: 100%|██████████| 4/4 [00:00<00:00, 178.25batch/s, Validation loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.011869291702168994 Final valid loss:  0.01103860605508089\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy\n",
    "# features from available initial features\n",
    "feature_extractor = make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "\n",
    "# regression model\n",
    "regression_model = get_regression_model()\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "x_train_transformed = feature_extractor(x_train).numpy()\n",
    "x_test_transformed = feature_extractor(x_test).numpy()\n",
    "\n",
    "\n",
    "regression_model.fit(x_train_transformed, y_train)\n",
    "y_pred = regression_model.predict(x_test_transformed)\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(\"results.csv\", index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}