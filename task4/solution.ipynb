{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n",
    "# First, we import necessary libraries:\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.linear_model import LinearRegression, Lasso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x19f0e9facb0>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"public/pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                         axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"public/pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"public/train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                   axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"public/train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"public/test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraining data\n",
    "        # and then used to extract features from the training and test data.\n",
    "\n",
    "        # activation = nn.ReLU()\n",
    "\n",
    "        # THINGS TO INCLUDE: BATCH NORM, DROPOUT\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 500), nn.BatchNorm1d(500),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250), nn.BatchNorm1d(250),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 90), nn.BatchNorm1d(90),\n",
    "            # nn.Sigmoid()\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(100, 50), nn.BatchNorm1d(50),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(50, 10), nn.BatchNorm1d(50),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(10, 50), nn.BatchNorm1d(50), nn.ReLU(),\n",
    "            # nn.Linear(50, 100), nn.BatchNorm1d(100), nn.ReLU(),\n",
    "            nn.Linear(90, 250), nn.BatchNorm1d(250),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 500), nn.BatchNorm1d(500),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 1000)\n",
    "        )\n",
    "        self.last_linear = nn.Sequential(nn.Linear(90, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture\n",
    "        # defined in the constructor.\n",
    "        x = self.encoder(x)\n",
    "        y = self.last_linear(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "\n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.train()\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set\n",
    "    # to monitor the loss.\n",
    "\n",
    "    criterion_decoded = nn.MSELoss()\n",
    "    criterion_predictions = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    n_epochs = 5\n",
    "    a = 0.1\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_tr, y_tr),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = []\n",
    "        valid_loss_epoch = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} train\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                decoded_features, predictions = model(data)\n",
    "                predictions = predictions.squeeze()\n",
    "\n",
    "                loss_decoded = criterion_decoded(decoded_features, data)\n",
    "                loss_predictions = criterion_predictions(predictions, target)\n",
    "\n",
    "                train_loss = a*loss_predictions + (1-a)*loss_decoded\n",
    "\n",
    "                train_loss_epoch.append(train_loss.item())\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tepoch.set_postfix({'Train loss': train_loss.item()})\n",
    "\n",
    "        train_loss_avg = np.mean(train_loss_epoch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "                for valid_data, valid_target in tepoch:\n",
    "                    tepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "\n",
    "                    valid_decoded_features, valid_predictions = model(valid_data)\n",
    "                    valid_predictions = valid_predictions.squeeze()\n",
    "\n",
    "                    valid_loss_decoded = criterion_decoded(valid_decoded_features, valid_data)\n",
    "                    valid_loss_predictions = criterion_predictions(valid_predictions, valid_target)\n",
    "\n",
    "                    valid_loss = a*valid_loss_predictions + (1-a)*valid_loss_decoded\n",
    "\n",
    "                    valid_loss_epoch.append(valid_loss.item())\n",
    "\n",
    "                    tepoch.set_postfix({'Validation loss': valid_loss.item()})\n",
    "\n",
    "        valid_loss_avg = np.mean(valid_loss_epoch)\n",
    "\n",
    "\n",
    "        losses.append(train_loss_avg)\n",
    "        valid_losses.append(valid_loss_avg)\n",
    "\n",
    "        print('Final train loss: ', train_loss_avg, 'Final valid loss: ', valid_loss_avg)\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline\n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        model_no_last_layers = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, pd.DataFrame):\n",
    "                x = x.to_numpy()\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            x_features = model_no_last_layers(x)\n",
    "\n",
    "        return x_features\n",
    "\n",
    "    return make_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train: 100%|██████████| 192/192 [00:04<00:00, 41.47batch/s, Train loss=0.0285]\n",
      "Epoch 0 valid: 100%|██████████| 4/4 [00:00<00:00, 96.35batch/s, Validation loss=0.0277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.21637873087699214 Final valid loss:  0.027751350309699774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|██████████| 192/192 [00:04<00:00, 42.12batch/s, Train loss=0.0242]\n",
      "Epoch 1 valid: 100%|██████████| 4/4 [00:00<00:00, 106.63batch/s, Validation loss=0.0248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.025789997608323272 Final valid loss:  0.02399823348969221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|██████████| 192/192 [00:05<00:00, 33.56batch/s, Train loss=0.0217]\n",
      "Epoch 2 valid: 100%|██████████| 4/4 [00:00<00:00, 78.43batch/s, Validation loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.022239563821737345 Final valid loss:  0.02078984398394823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|██████████| 192/192 [00:06<00:00, 29.57batch/s, Train loss=0.0192]\n",
      "Epoch 3 valid: 100%|██████████| 4/4 [00:00<00:00, 79.17batch/s, Validation loss=0.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.019683974765939638 Final valid loss:  0.01948577957227826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|██████████| 192/192 [00:06<00:00, 30.73batch/s, Train loss=0.0169]\n",
      "Epoch 4 valid: 100%|██████████| 4/4 [00:00<00:00, 82.44batch/s, Validation loss=0.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.01753880308630566 Final valid loss:  0.017389880493283272\n"
     ]
    }
   ],
   "source": [
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy\n",
    "# features from available initial features\n",
    "feature_extractor = make_feature_extractor(x_pretrain, y_pretrain)\n",
    "\n",
    "x_train_transformed = feature_extractor(x_train).numpy()\n",
    "x_test_transformed = feature_extractor(x_test).numpy()\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "# STANDARDSCALER, FUNCTIONTRANSFORMER, etc.\n",
    "# x_train_transformed = scale(x_train_transformed)\n",
    "# x_test_transformed = scale(x_test_transformed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0\n",
      "-0.490482071318851\n",
      "alpha = 0.001\n",
      "-0.23444284915176922\n",
      "alpha = 0.01\n",
      "-0.16603550085346433\n",
      "alpha = 0.05\n",
      "-0.15191928117631673\n",
      "alpha = 0.1\n",
      "-0.1511619747964349\n",
      "alpha = 0.5\n",
      "-0.1502997804813138\n",
      "alpha = 1\n",
      "-0.15078359663241184\n",
      "alpha = 5\n",
      "-0.15995668548899838\n",
      "alpha = 10\n",
      "-0.17120536150849006\n",
      "alpha = 100\n",
      "-0.21013934673300597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "print('alpha =', 0)\n",
    "pprint(np.mean(cross_val_score(LinearRegression(), x_train_transformed, y_train, cv=LeaveOneOut(), scoring='neg_root_mean_squared_error')))\n",
    "for alpha in [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100]:\n",
    "    print('alpha =', alpha)\n",
    "    pprint(np.mean(cross_val_score(Ridge(alpha=alpha), x_train_transformed, y_train, cv=LeaveOneOut(), scoring='neg_root_mean_squared_error')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "regression_model = Ridge(alpha=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "regression_model.fit(x_train_transformed, y_train)\n",
    "y_pred = regression_model.predict(x_test_transformed)\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(\"results.csv\", index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "iml-2023",
   "language": "python",
   "display_name": "Python (iml-2023)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}