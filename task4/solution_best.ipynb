{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n",
    "# First, we import necessary libraries:\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19e366d6830>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"public/pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                         axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"public/pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"public/train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                   axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"public/train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"public/test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraining data\n",
    "        # and then used to extract features from the training and test data.\n",
    "\n",
    "        # activation = nn.ReLU()\n",
    "\n",
    "        # THINGS TO INCLUDE: BATCH NORM, DROPOUT\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 500), nn.BatchNorm1d(500),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250), nn.BatchNorm1d(250),\n",
    "            nn.Dropout(0.3),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 90), nn.BatchNorm1d(90),\n",
    "            # nn.Sigmoid()\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(100, 50), nn.BatchNorm1d(50),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(50, 10), nn.BatchNorm1d(50),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(10, 50), nn.BatchNorm1d(50), nn.ReLU(),\n",
    "            # nn.Linear(50, 100), nn.BatchNorm1d(100), nn.ReLU(),\n",
    "            nn.Linear(90, 250), nn.BatchNorm1d(250),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 500), nn.BatchNorm1d(500),\n",
    "            nn.Dropout(0.3),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 1000)\n",
    "        )\n",
    "        self.last_linear = nn.Sequential(nn.Linear(90, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture\n",
    "        # defined in the constructor.\n",
    "        x = self.encoder(x)\n",
    "        y = self.last_linear(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "\n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.train()\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set\n",
    "    # to monitor the loss.\n",
    "\n",
    "    criterion_decoded = nn.MSELoss()\n",
    "    criterion_predictions = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    n_epochs = 5\n",
    "    a = 0.15\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_tr, y_tr),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = []\n",
    "        valid_loss_epoch = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} train\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                decoded_features, predictions = model(data)\n",
    "                predictions = predictions.squeeze()\n",
    "\n",
    "                loss_decoded = criterion_decoded(decoded_features, data)\n",
    "                loss_predictions = criterion_predictions(predictions, target)\n",
    "\n",
    "                train_loss = a*loss_predictions + (1-a)*loss_decoded\n",
    "\n",
    "                train_loss_epoch.append(train_loss.item())\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tepoch.set_postfix({'Train loss': train_loss.item()})\n",
    "\n",
    "        train_loss_avg = np.mean(train_loss_epoch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "                for valid_data, valid_target in tepoch:\n",
    "                    tepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "\n",
    "                    valid_decoded_features, valid_predictions = model(valid_data)\n",
    "                    valid_predictions = valid_predictions.squeeze()\n",
    "\n",
    "                    valid_loss_decoded = criterion_decoded(valid_decoded_features, valid_data)\n",
    "                    valid_loss_predictions = criterion_predictions(valid_predictions, valid_target)\n",
    "\n",
    "                    valid_loss = a*valid_loss_predictions + (1-a)*valid_loss_decoded\n",
    "\n",
    "                    valid_loss_epoch.append(valid_loss.item())\n",
    "\n",
    "                    tepoch.set_postfix({'Validation loss': valid_loss.item()})\n",
    "\n",
    "        valid_loss_avg = np.mean(valid_loss_epoch)\n",
    "\n",
    "\n",
    "        losses.append(train_loss_avg)\n",
    "        valid_losses.append(valid_loss_avg)\n",
    "\n",
    "        print('Final train loss: ', train_loss_avg, 'Final valid loss: ', valid_loss_avg)\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline\n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        model_no_last_layers = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, pd.DataFrame):\n",
    "                x = x.to_numpy()\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            x_features = model_no_last_layers(x)\n",
    "\n",
    "        return x_features\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train: 100%|████████████████████████████████████████████| 192/192 [02:09<00:00,  1.48batch/s, Train loss=0.028]\n",
      "Epoch 0 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.43batch/s, Validation loss=0.0286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.3239965158281848 Final valid loss:  0.028138712979853153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|███████████████████████████████████████████| 192/192 [02:11<00:00,  1.46batch/s, Train loss=0.0256]\n",
      "Epoch 1 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.49batch/s, Validation loss=0.0233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.02540137601317838 Final valid loss:  0.02366438927128911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|███████████████████████████████████████████| 192/192 [02:13<00:00,  1.44batch/s, Train loss=0.0238]\n",
      "Epoch 2 valid: 100%|███████████████████████████████████████████| 4/4 [00:01<00:00,  2.43batch/s, Validation loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.022270441753789783 Final valid loss:  0.021124161779880524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|███████████████████████████████████████████| 192/192 [02:14<00:00,  1.42batch/s, Train loss=0.0215]\n",
      "Epoch 3 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.42batch/s, Validation loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.020522422913927585 Final valid loss:  0.019714676775038242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|███████████████████████████████████████████| 192/192 [02:13<00:00,  1.44batch/s, Train loss=0.0197]\n",
      "Epoch 4 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.57batch/s, Validation loss=0.0189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.019388026198915515 Final valid loss:  0.01902792789041996\n"
     ]
    }
   ],
   "source": [
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy\n",
    "# features from available initial features\n",
    "feature_extractor = make_feature_extractor(x_pretrain, y_pretrain)\n",
    "\n",
    "x_train_transformed = feature_extractor(x_train).numpy()\n",
    "x_test_transformed = feature_extractor(x_test).numpy()\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "# STANDARDSCALER, FUNCTIONTRANSFORMER, etc.\n",
    "# x_train_transformed = scale(x_train_transformed)\n",
    "# x_test_transformed = scale(x_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "print('alpha =', 0)\n",
    "pprint(np.mean(cross_val_score(LinearRegression(), x_train_transformed, y_train, cv=LeaveOneOut(), scoring='neg_root_mean_squared_error')))\n",
    "for alpha in [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100]:\n",
    "    print('alpha =', alpha)\n",
    "    pprint(np.mean(cross_val_score(Ridge(alpha=alpha), x_train_transformed, y_train, cv=LeaveOneOut(), scoring='neg_root_mean_squared_error')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regression_model = Ridge(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "regression_model.fit(x_train_transformed, y_train)\n",
    "y_pred = regression_model.predict(x_test_transformed)\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(\"results.csv\", index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train: 100%|███████████████████████████████████████████| 192/192 [02:12<00:00,  1.45batch/s, Train loss=0.0306]\n",
      "Epoch 0 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.47batch/s, Validation loss=0.0306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.24404698217404075 Final valid loss:  0.02892347238957882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|███████████████████████████████████████████| 192/192 [02:16<00:00,  1.40batch/s, Train loss=0.0254]\n",
      "Epoch 1 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.35batch/s, Validation loss=0.0259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.02725943362262721 Final valid loss:  0.02600444434210658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|███████████████████████████████████████████| 192/192 [02:15<00:00,  1.42batch/s, Train loss=0.0229]\n",
      "Epoch 2 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.48batch/s, Validation loss=0.0237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.024079614008466404 Final valid loss:  0.023205572739243507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|███████████████████████████████████████████| 192/192 [02:19<00:00,  1.38batch/s, Train loss=0.0208]\n",
      "Epoch 3 valid: 100%|██████████████████████████████████████████| 4/4 [00:01<00:00,  2.42batch/s, Validation loss=0.0208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.02192225185960221 Final valid loss:  0.020878405775874853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|█████████████████████████████████████████████| 192/192 [02:20<00:00,  1.37batch/s, Train loss=0.02]\n",
      "Epoch 4 valid: 100%|███████████████████████████████████████████| 4/4 [00:01<00:00,  2.39batch/s, Validation loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss:  0.020199773027949657 Final valid loss:  0.02079623658210039\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n",
    "# First, we import necessary libraries:\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"public/pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                         axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"public/pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"public/train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\",\n",
    "                                                                                                   axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"public/train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"public/test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraining data\n",
    "        # and then used to extract features from the training and test data.\n",
    "\n",
    "        # activation = nn.ReLU()\n",
    "\n",
    "        # THINGS TO INCLUDE: BATCH NORM, DROPOUT\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 500), nn.BatchNorm1d(500),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250), nn.BatchNorm1d(250),\n",
    "            nn.Dropout(0.3),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 90), nn.BatchNorm1d(90),\n",
    "            # nn.Sigmoid()\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(100, 50), nn.BatchNorm1d(50),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(50, 10), nn.BatchNorm1d(50),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(10, 50), nn.BatchNorm1d(50), nn.ReLU(),\n",
    "            # nn.Linear(50, 100), nn.BatchNorm1d(100), nn.ReLU(),\n",
    "            nn.Linear(90, 250), nn.BatchNorm1d(250),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 500), nn.BatchNorm1d(500),\n",
    "            nn.Dropout(0.3),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 1000)\n",
    "        )\n",
    "        self.last_linear = nn.Sequential(nn.Linear(90, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture\n",
    "        # defined in the constructor.\n",
    "        x = self.encoder(x)\n",
    "        y = self.last_linear(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "\n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.train()\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set\n",
    "    # to monitor the loss.\n",
    "\n",
    "    criterion_decoded = nn.MSELoss()\n",
    "    criterion_predictions = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    n_epochs = 5\n",
    "    a = 0.15\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_tr, y_tr),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=TensorDataset(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = []\n",
    "        valid_loss_epoch = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} train\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                decoded_features, predictions = model(data)\n",
    "                predictions = predictions.squeeze()\n",
    "\n",
    "                loss_decoded = criterion_decoded(decoded_features, data)\n",
    "                loss_predictions = criterion_predictions(predictions, target)\n",
    "\n",
    "                train_loss = a*loss_predictions + (1-a)*loss_decoded\n",
    "\n",
    "                train_loss_epoch.append(train_loss.item())\n",
    "\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tepoch.set_postfix({'Train loss': train_loss.item()})\n",
    "\n",
    "        train_loss_avg = np.mean(train_loss_epoch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "                for valid_data, valid_target in tepoch:\n",
    "                    tepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "\n",
    "                    valid_decoded_features, valid_predictions = model(valid_data)\n",
    "                    valid_predictions = valid_predictions.squeeze()\n",
    "\n",
    "                    valid_loss_decoded = criterion_decoded(valid_decoded_features, valid_data)\n",
    "                    valid_loss_predictions = criterion_predictions(valid_predictions, valid_target)\n",
    "\n",
    "                    valid_loss = a*valid_loss_predictions + (1-a)*valid_loss_decoded\n",
    "\n",
    "                    valid_loss_epoch.append(valid_loss.item())\n",
    "\n",
    "                    tepoch.set_postfix({'Validation loss': valid_loss.item()})\n",
    "\n",
    "        valid_loss_avg = np.mean(valid_loss_epoch)\n",
    "\n",
    "\n",
    "        losses.append(train_loss_avg)\n",
    "        valid_losses.append(valid_loss_avg)\n",
    "\n",
    "        print('Final train loss: ', train_loss_avg, 'Final valid loss: ', valid_loss_avg)\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline\n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        model_no_last_layers = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, pd.DataFrame):\n",
    "                x = x.to_numpy()\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            x_features = model_no_last_layers(x)\n",
    "\n",
    "        return x_features\n",
    "\n",
    "    return make_features\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "    print(\"Data loaded!\")\n",
    "    \n",
    "    # Utilize pretraining data by creating feature extractor which extracts lumo energy\n",
    "    # features from available initial features\n",
    "    feature_extractor = make_feature_extractor(x_pretrain, y_pretrain)\n",
    "\n",
    "    x_train_transformed = feature_extractor(x_train).numpy()\n",
    "    x_test_transformed = feature_extractor(x_test).numpy()\n",
    "\n",
    "    y_pred = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    regression_model = Ridge(alpha=0.5)\n",
    "    \n",
    "    regression_model.fit(x_train_transformed, y_train)\n",
    "    y_pred = regression_model.predict(x_test_transformed)\n",
    "\n",
    "    assert y_pred.shape == (x_test.shape[0],)\n",
    "    y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "    y_pred.to_csv(\"results.csv\", index_label=\"Id\")\n",
    "    print(\"Predictions saved, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
