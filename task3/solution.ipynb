{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 70.513285,
   "end_time": "2023-05-03T14:02:16.882503",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-03T14:01:06.369218",
   "version": "2.4.0"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.004202,
     "end_time": "2023-05-03T14:01:16.627220",
     "exception": false,
     "start_time": "2023-05-03T14:01:16.623018",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "seed = 111\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ],
   "metadata": {
    "papermill": {
     "duration": 5.069795,
     "end_time": "2023-05-03T14:01:21.701036",
     "exception": false,
     "start_time": "2023-05-03T14:01:16.631241",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-05-09T07:54:29.761896Z",
     "iopub.execute_input": "2023-05-09T07:54:29.762332Z",
     "iopub.status.idle": "2023-05-09T07:54:36.134109Z",
     "shell.execute_reply.started": "2023-05-09T07:54:29.762302Z",
     "shell.execute_reply": "2023-05-09T07:54:36.133055Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "cuda:0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Data + EDA"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.00363,
     "end_time": "2023-05-03T14:01:21.709183",
     "exception": false,
     "start_time": "2023-05-03T14:01:21.705553",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "def generate_embeddings(batch_size):\n",
    "    \"\"\"\n",
    "    Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "    the embeddings.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    \n",
    "    # TODO: define a transform to pre-process the images\n",
    "    train_transforms = transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    #torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]) # tune this\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=\"/kaggle/input/imltask3/dataset/dataset/\", transform=train_transforms)\n",
    "    # Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "    # run out of memory\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size, # tune this\n",
    "                              shuffle=False,\n",
    "                              pin_memory=True, num_workers=2)\n",
    "    # TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n",
    "    #  more info here: https://pytorch.org/vision/stable/models.html)\n",
    "    model = resnet50(weights=\"IMAGENET1K_V2\")\n",
    "    # model.to(device)\n",
    "\n",
    "    embeddings = []\n",
    "    embedding_size = list(model.children())[-1].in_features # 2048\n",
    "    num_images = len(train_dataset)\n",
    "    embeddings = np.zeros((num_images, embedding_size))\n",
    "\n",
    "    # TODO: Use the model to extract the embeddings. Hint: remove the last layers of the \n",
    "    # model to access the embeddings the model generates.\n",
    "     \n",
    "    model.eval() \n",
    "\n",
    "    model = nn.Sequential(*list(model.children())[:-1]) # remove last layer of the model\n",
    "\n",
    "    print('Extracting features:')\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (image, image_idx) in enumerate(tqdm(train_loader)):\n",
    "            embed_features = model(image) # get features from pretrained model  \n",
    "            embed_features = embed_features.squeeze().cpu().numpy() # get to shape (256, 2048)\n",
    "            embeddings[batch_idx * train_loader.batch_size : (batch_idx + 1) * train_loader.batch_size] = embed_features           \n",
    "            \n",
    "    np.save(f'embeddings_{batch_size}_cropped.npy', embeddings)\n",
    "    \n",
    "#generate_embeddings(batch_size=768) # -> both embeddings code and modelling do not work bc of memory issues"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.021298,
     "end_time": "2023-05-03T14:01:21.734427",
     "exception": false,
     "start_time": "2023-05-03T14:01:21.713129",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-05-09T07:54:36.136448Z",
     "iopub.execute_input": "2023-05-09T07:54:36.137085Z",
     "iopub.status.idle": "2023-05-09T07:54:36.148594Z",
     "shell.execute_reply.started": "2023-05-09T07:54:36.137049Z",
     "shell.execute_reply": "2023-05-09T07:54:36.147374Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\nWall time: 10 µs\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    # generate training data from tripletsfiles = os.listdir(os.path.join(inputfolder,'dataset/food'))\n",
    "    #files = os.listdir(os.path.join('dataset/food'))\n",
    "    #filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    \n",
    "    filenames = np.loadtxt(\"/kaggle/input/imltask3/filenames.txt\", dtype=str) # if run on kaggle\n",
    "    embeddings = np.load('/kaggle/input/imltask3/embeddings_512.npy') # if run on kaggle\n",
    "    # TODO: Normalize the embeddings across the dataset\n",
    "    \n",
    "    embeddings = (embeddings - np.mean(embeddings, axis=0)) / np.std(embeddings, axis=0)\n",
    "    \n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_name = filenames[i]\n",
    "        file_to_embedding[file_name] = embeddings[i]\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in tqdm(triplets):\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation) \n",
    "            # -> basically swap image1 with image2 which will get output 0\n",
    "                # can we augment it even more? (not sure)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "TRAIN_TRIPLETS = '/kaggle/input/imltask3/train_triplets.txt'\n",
    "TEST_TRIPLETS = '/kaggle/input/imltask3/test_triplets.txt'\n",
    "\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train = X_train.reshape(-1, 512, 4, 3) # resize to 4dTensor for CNNs, maybe correct shape is (-1, 256, 6, 4), not sure\n",
    "X_valid = X_valid.reshape(-1, 512, 4, 3)\n",
    "X_test, _ = get_data(TEST_TRIPLETS, train=False)\n",
    "X_test = X_test.reshape(-1, 512, 4, 3) # resize to 4dTensor for CNNs"
   ],
   "metadata": {
    "papermill": {
     "duration": 14.996118,
     "end_time": "2023-05-03T14:01:36.734477",
     "exception": false,
     "start_time": "2023-05-03T14:01:21.738359",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-05-09T07:54:36.151137Z",
     "iopub.execute_input": "2023-05-09T07:54:36.153418Z",
     "iopub.status.idle": "2023-05-09T07:54:45.468164Z",
     "shell.execute_reply.started": "2023-05-09T07:54:36.151452Z",
     "shell.execute_reply": "2023-05-09T07:54:45.467211Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 59515/59515 [00:04<00:00, 14819.41it/s]\n100%|██████████| 59544/59544 [00:00<00:00, 78547.85it/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create loader"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.007965,
     "end_time": "2023-05-03T14:01:36.751965",
     "exception": false,
     "start_time": "2023-05-03T14:01:36.744000",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory\n",
    "def create_loader_from_np(X, y = None, train = True, batch_size=64, shuffle=True, num_workers = 2):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    print(\"Load data\")\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float),\n",
    "                                torch.from_numpy(y).type(torch.long))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "train_loader = create_loader_from_np(X_train, y_train, train = True, batch_size=64)\n",
    "valid_loader = create_loader_from_np(X_valid, y_valid, train = True, batch_size=64)\n",
    "test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.036783,
     "end_time": "2023-05-03T14:01:36.797115",
     "exception": false,
     "start_time": "2023-05-03T14:01:36.760332",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-05-09T07:54:45.470559Z",
     "iopub.execute_input": "2023-05-09T07:54:45.470906Z",
     "iopub.status.idle": "2023-05-09T07:54:45.500721Z",
     "shell.execute_reply.started": "2023-05-09T07:54:45.470873Z",
     "shell.execute_reply": "2023-05-09T07:54:45.499562Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Load data\nLoad data\nLoad data\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.007913,
     "end_time": "2023-05-03T14:01:36.813209",
     "exception": false,
     "start_time": "2023-05-03T14:01:36.805296",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: define a model. Here, the basic structure is defined, but you need to fill in the details\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=True, dropout_p=0.5): #0.4 -> tune this\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convlayer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.convlayer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        )\n",
    "        self.convlayer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        self.fullycon1 = nn.Sequential(nn.Linear(8 * 2 * 1, 120), nn.ReLU())\n",
    "        self.fullycon2 = nn.Sequential(nn.Linear(120, 84), nn.ReLU())\n",
    "        if dropout:\n",
    "            self.fullycon3 = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(84, 64))\n",
    "        else:\n",
    "            self.fullycon3 = nn.Linear(84, 64)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.convlayer2(x)\n",
    "        x = self.convlayer3(x)\n",
    "        x = x.view(-1, 8 * 2 * 1)\n",
    "        x = self.fullycon1(x)\n",
    "        x = self.fullycon2(x)\n",
    "        x = self.fullycon3(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_loader):\n",
    "    \"\"\"\n",
    "    The training procedure of the model; it accepts the training data, defines the model \n",
    "    and then trains it.\n",
    "\n",
    "    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "    output: model: torch.nn.Module, the trained model\n",
    "    \"\"\"\n",
    "    model = Net()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    print('device: ', device)\n",
    "    n_epochs = 1 # tune this\n",
    "    batch_size = 256 # and this\n",
    "\n",
    "    losses = []\n",
    "    acc = []\n",
    "    valid_losses = []\n",
    "    valid_acc = []\n",
    "    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "    # of the training data as a validation split. After each epoch, compute the loss on the \n",
    "    # validation\n",
    "    # split and print it out. This enables you to see how your model is performing \n",
    "    # on the validation data before submitting the results on the server. After choosing the \n",
    "    # best model, train it on the whole training data.\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) # tune lr\n",
    "\n",
    "    print(\"Train model\")\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = []\n",
    "        train_acc_epoch = []\n",
    "        valid_loss_epoch = []\n",
    "        valid_acc_epoch = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "                train_loss_epoch.append(loss.item())\n",
    "                correct = (predictions == target).sum().item()\n",
    "                accuracy = correct / len(predictions)\n",
    "                train_acc_epoch.append(accuracy)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss_avg = np.sum(train_loss_epoch) / len(train_loss_epoch)\n",
    "                train_acc_avg = np.sum(train_acc_epoch) / len(train_acc_epoch)\n",
    "                tepoch.set_postfix({'Train loss': train_loss_avg, 'Train accuracy': 100. * train_acc_avg})\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "                    for valid_data, valid_target in tepoch:\n",
    "                        tepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "                        valid_data, valid_target = valid_data.to(device), valid_target.to(device)\n",
    "                        valid_output = model(valid_data)\n",
    "                        valid_predictions = valid_output.argmax(dim=1, keepdim=True).squeeze()\n",
    "                        valid_loss = criterion(valid_output, valid_target)\n",
    "                        valid_loss_epoch.append(valid_loss.item())\n",
    "                        valid_correct = (valid_predictions == valid_target).sum().item()\n",
    "                        valid_accuracy = valid_correct / len(valid_predictions)\n",
    "                        valid_acc_epoch.append(valid_accuracy)\n",
    "                        \n",
    "                        valid_loss_avg = np.sum(valid_loss_epoch) / len(valid_loss_epoch)\n",
    "                        valid_acc_avg = np.sum(valid_acc_epoch) / len(valid_acc_epoch)\n",
    "                        tepoch.set_postfix({'Val loss': valid_loss.item(), 'Val accuracy': 100. * valid_accuracy})\n",
    "        \n",
    "        losses.append(train_loss_avg)\n",
    "        acc.append(train_acc_avg)\n",
    "        valid_losses.append(valid_loss_avg)\n",
    "        valid_acc.append(valid_acc_avg)\n",
    "        \n",
    "        print('Final train accuracy: ', train_acc_avg, 'Final valid accuracy: ', valid_acc_avg,\n",
    "             '\\n Final train loss: ', train_loss_avg, 'Final valid loss: ', valid_loss_avg)\n",
    "        \n",
    "    return model, losses, acc, valid_losses, valid_acc\n",
    "\n",
    "model, losses, acc, valid_losses, valid_acc = train_model(train_loader)"
   ],
   "metadata": {
    "papermill": {
     "duration": 31.798446,
     "end_time": "2023-05-03T14:02:08.619884",
     "exception": false,
     "start_time": "2023-05-03T14:01:36.821438",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-05-09T07:54:45.503535Z",
     "iopub.execute_input": "2023-05-09T07:54:45.504401Z",
     "iopub.status.idle": "2023-05-09T07:55:20.695214Z",
     "shell.execute_reply.started": "2023-05-09T07:54:45.504342Z",
     "shell.execute_reply": "2023-05-09T07:55:20.693981Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "device:  cuda:0\nTrain model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Epoch 0: 100%|██████████| 1488/1488 [00:27<00:00, 53.56batch/s, Train loss=0.594, Train accuracy=68.5]\nEpoch 0 valid: 100%|██████████| 372/372 [00:03<00:00, 113.46batch/s, Val loss=0.45, Val accuracy=82.3] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Final train accuracy:  0.6852558563748079 Final valid accuracy:  0.7386863727020464 \n Final train loss:  0.5940255130170494 Final valid loss:  0.5299263106238458\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=losses,\n",
    "    name = 'Train Loss',\n",
    "    connectgaps=True\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=acc,\n",
    "    name='Train Accuracy',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=valid_losses,\n",
    "    name = 'Valid Loss',\n",
    "    connectgaps=True\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=valid_acc,\n",
    "    name='Valid Accuracy',\n",
    "))\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.546108,
     "end_time": "2023-05-03T14:02:09.420063",
     "exception": false,
     "start_time": "2023-05-03T14:02:08.873955",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-05-09T07:55:20.697137Z",
     "iopub.execute_input": "2023-05-09T07:55:20.697559Z",
     "iopub.status.idle": "2023-05-09T07:55:21.090319Z",
     "shell.execute_reply.started": "2023-05-09T07:55:20.697517Z",
     "shell.execute_reply": "2023-05-09T07:55:21.089348Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": "        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": "<div>                            <div id=\"366a58fe-a258-45df-a7f8-376198ff18be\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"366a58fe-a258-45df-a7f8-376198ff18be\")) {                    Plotly.newPlot(                        \"366a58fe-a258-45df-a7f8-376198ff18be\",                        [{\"connectgaps\":true,\"name\":\"Train Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],\"y\":[0.5940255130170494],\"type\":\"scatter\"},{\"name\":\"Train Accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],\"y\":[0.6852558563748079],\"type\":\"scatter\"},{\"connectgaps\":true,\"name\":\"Valid Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],\"y\":[0.5299263106238458],\"type\":\"scatter\"},{\"name\":\"Valid Accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],\"y\":[0.7386863727020464],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('366a58fe-a258-45df-a7f8-376198ff18be');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test model"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.322119,
     "end_time": "2023-05-03T14:02:09.992199",
     "exception": false,
     "start_time": "2023-05-03T14:02:09.670080",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def test_model(model, loader):\n",
    "    \"\"\"\n",
    "    The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "    then tests the model on it.\n",
    "\n",
    "    input: model: torch.nn.Module, the trained model\n",
    "           loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "    output: None, the function saves the predictions to a results.txt file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "        for [x_batch] in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.argmax(dim=1, keepdim=True).squeeze().cpu().numpy()\n",
    "            predicted[predicted >= 0.5] = 1\n",
    "            predicted[predicted < 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.hstack(predictions)\n",
    "    np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "    \n",
    "test_model(model, test_loader)\n",
    "print(\"Results saved to results.txt\")"
   ],
   "metadata": {
    "papermill": {
     "duration": 3.961751,
     "end_time": "2023-05-03T14:02:14.202783",
     "exception": false,
     "start_time": "2023-05-03T14:02:10.241032",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-05-09T07:55:27.113071Z",
     "iopub.execute_input": "2023-05-09T07:55:27.113499Z",
     "iopub.status.idle": "2023-05-09T07:55:29.678205Z",
     "shell.execute_reply.started": "2023-05-09T07:55:27.113463Z",
     "shell.execute_reply": "2023-05-09T07:55:29.676400Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Results saved to results.txt\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "papermill": {
     "duration": 0.252292,
     "end_time": "2023-05-03T14:02:14.704979",
     "exception": false,
     "start_time": "2023-05-03T14:02:14.452687",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}